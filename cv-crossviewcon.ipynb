{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13824452,"sourceType":"datasetVersion","datasetId":8803863},{"sourceId":13824629,"sourceType":"datasetVersion","datasetId":8803995}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random, sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\ntf.keras.backend.clear_session()\n\nMODE = 'fusion'\n\n\nMOJ_ROOT = '/kaggle/input/n-ucla-moj'     \nED_MHI_ROOT = '/kaggle/input/n-ucla-edmhi'      \n\n# training params\nSEED = 0\nBATCH_SIZE = 8\nEPOCHS_HEAD = 10      \nEPOCHS_FINETUNE = 40   \nLR_HEAD = 1e-3\n# LR_HEAD = 1e-4\nLR_FINETUNE = 5e-5\nLAMBDA_CV_MOJ = 0.01\nLAMBDA_CV_ED = 0.1    \nTARGET_SIZE = (224, 224)\nNUM_WORKERS = 2\n\nPATIENCE = 8\nMODEL_SAVE_PATH = \"/kaggle/working/best_cvcl_model.h5\"\n\nDETERMINISTIC = False\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:06:27.540502Z","iopub.execute_input":"2025-11-25T07:06:27.541308Z","iopub.status.idle":"2025-11-25T07:06:27.735423Z","shell.execute_reply.started":"2025-11-25T07:06:27.541279Z","shell.execute_reply":"2025-11-25T07:06:27.734842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\ndef parse_subject_id(fname):\n    m = re.search(r\"s\\d{2}\", fname)\n    return m.group(0) if m else None\n\ndef parse_action_id(fname):\n    m = re.match(r\"(a\\d{2}_s\\d{2}_e\\d{2})\", fname)\n    return m.group(1) if m else None\n\ndef collect_samples(root_dir):\n    \"\"\"Return dict view -> {aid: (path, subject)}\"\"\"\n    view_samples = {}\n    for view in [\"view_1\",\"view_2\",\"view_3\"]:\n        view_path = os.path.join(root_dir, view)\n        files = [f for f in os.listdir(view_path) if f.lower().endswith((\".jpg\",\".png\"))]\n        samples = {}\n        for f in files:\n            sid = parse_subject_id(f)\n            aid = parse_action_id(f)\n            if sid and aid:\n                samples[aid] = (os.path.join(view_path,f), sid)\n        view_samples[view] = samples\n    return view_samples\n\ndef intersect_views(view_samples):\n    return set(view_samples[\"view_1\"].keys()) & set(view_samples[\"view_2\"].keys()) & set(view_samples[\"view_3\"].keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:06:34.416540Z","iopub.execute_input":"2025-11-25T07:06:34.416882Z","iopub.status.idle":"2025-11-25T07:06:34.424028Z","shell.execute_reply.started":"2025-11-25T07:06:34.416856Z","shell.execute_reply":"2025-11-25T07:06:34.423151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport re\ndef parse_subject_id(fname):\n    m = re.search(r\"s\\d{2}\", fname)\n    return m.group(0) if m else None\n\ndef parse_action_id(fname):\n    m = re.match(r\"(a\\d{2}_s\\d{2}_e\\d{2})\", fname)\n    return m.group(1) if m else None\n\ndef collect_samples(root_dir):\n    \"\"\"Return dict view -> {aid: (path, subject)}\"\"\"\n    view_samples = {}\n    for view in [\"view_1\",\"view_2\",\"view_3\"]:\n        view_path = os.path.join(root_dir, view)\n        files = [f for f in os.listdir(view_path) if f.lower().endswith((\".jpg\",\".png\"))]\n        samples = {}\n        for f in files:\n            sid = parse_subject_id(f)\n            aid = parse_action_id(f)\n            if sid and aid:\n                samples[aid] = (os.path.join(view_path,f), sid)\n        view_samples[view] = samples\n    return view_samples\n\ndef intersect_views(view_samples):\n    return set(view_samples[\"view_1\"].keys()) & set(view_samples[\"view_2\"].keys()) & set(view_samples[\"view_3\"].keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:06:42.660499Z","iopub.execute_input":"2025-11-25T07:06:42.660980Z","iopub.status.idle":"2025-11-25T07:06:42.667452Z","shell.execute_reply.started":"2025-11-25T07:06:42.660954Z","shell.execute_reply":"2025-11-25T07:06:42.666705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image(path, target_size=(224,224), is_moj=True):\n    if is_moj:\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, target_size)\n        img = img.astype(\"float32\") / 255.0\n    else:\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, target_size)\n        img = np.expand_dims(img, axis=-1)\n        img = img.astype(\"float32\")\n        img = img / (img.max() + 1e-6)   # normalize\n\n    return img\n\ndef build_triplet_dataset(moj_samples, ed_mhi_samples, sample_list, target_size=(224,224), le=None):\n    X1_moj, X2_moj, X3_moj = [], [], []\n    X1_ed,  X2_ed,  X3_ed  = [], [], []\n    y = []\n    for aid in tqdm(sample_list, desc=\"Building triplet dataset\"):\n        p1_moj, _ = moj_samples[\"view_1\"][aid]\n        p2_moj, _ = moj_samples[\"view_2\"][aid]\n        p3_moj, _ = moj_samples[\"view_3\"][aid]\n        X1_moj.append(load_image(p1_moj, target_size, True))\n        X2_moj.append(load_image(p2_moj, target_size, True))\n        X3_moj.append(load_image(p3_moj, target_size, True))\n        p1_ed, _ = ed_mhi_samples[\"view_1\"][aid]\n        p2_ed, _ = ed_mhi_samples[\"view_2\"][aid]\n        p3_ed, _ = ed_mhi_samples[\"view_3\"][aid]\n        X1_ed.append(load_image(p1_ed, target_size, False))\n        X2_ed.append(load_image(p2_ed, target_size, False))\n        X3_ed.append(load_image(p3_ed, target_size, False))\n        y.append(aid.split(\"_\")[0])\n    if le is None:\n        le = LabelEncoder()\n        y_enc = le.fit_transform(y)\n    else:\n        y_enc = le.transform(y)\n    num_classes = len(le.classes_)\n    y_cat = to_categorical(y_enc, num_classes=num_classes)\n    return (np.array(X1_moj), np.array(X2_moj), np.array(X3_moj),\n            np.array(X1_ed), np.array(X2_ed), np.array(X3_ed),\n            y_cat, le, num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:07:27.596103Z","iopub.execute_input":"2025-11-25T07:07:27.596720Z","iopub.status.idle":"2025-11-25T07:07:27.605331Z","shell.execute_reply.started":"2025-11-25T07:07:27.596693Z","shell.execute_reply":"2025-11-25T07:07:27.604570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_augmentation_layer():\n    return tf.keras.Sequential([\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.08),\n        layers.RandomZoom(0.08),\n        layers.RandomTranslation(0.05, 0.05),\n    ], name=\"data_augmentation\")\ndef get_augmentation_layer_ed():\n    return tf.keras.Sequential([\n        layers.RandomFlip(\"horizontal\")\n    ])\n\ndef build_encoder_rgb(input_shape=(224,224,3), use_augment=True, dropout_rate=0.3):\n    inp = layers.Input(shape=input_shape)\n    x = inp\n    if use_augment:\n        x = get_augmentation_layer()(x)\n    x = layers.Conv2D(32, (7,7), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Conv2D(64, (5,5), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Conv2D(256, (3,3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.LayerNormalization()(x)\n    return tf.keras.Model(inp, x, name=\"encoder_rgb\")\n\ndef build_encoder_gray(input_shape=(224,224,1), use_augment=False, dropout_rate=0.3):\n    inp = layers.Input(shape=input_shape)\n    x = inp\n    if use_augment:\n        x = get_augmentation_layer_ed()(x)\n    x = layers.Conv2D(64, (7,7), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Conv2D(128, (5,5), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Conv2D(256, (3,3), activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.LayerNormalization()(x)\n    return tf.keras.Model(inp, x, name=\"encoder_gray\")\n\ndef build_classifier(num_classes, input_dim=256):\n    inp = layers.Input(shape=(input_dim,))\n    x = layers.Dense(256, activation='relu')(inp)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    out = layers.Dense(num_classes, activation='softmax')(x)\n    return tf.keras.Model(inp, out, name=\"classifier\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:34:48.967484Z","iopub.execute_input":"2025-11-25T07:34:48.967759Z","iopub.status.idle":"2025-11-25T07:34:48.978929Z","shell.execute_reply.started":"2025-11-25T07:34:48.967734Z","shell.execute_reply":"2025-11-25T07:34:48.978184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossViewModel(tf.keras.Model):\n    \"\"\"\n    mode: 'moj' | 'ed' | 'fusion'\n    - if 'moj': encoder_rgb used for x1,x2,x3\n    - if 'ed': encoder_gray used for x1,x2,x3\n    - if 'fusion': both encoders used, fusion on averaged features\n    \"\"\"\n    def reset_metrics(self):\n        self.train_acc.reset_state()\n        self.val_acc.reset_state()\n        self.cv_tracker.reset_state()\n\n\n    def __init__(self, encoder_rgb=None, encoder_gray=None, classifier=None,\n                 mode='moj', lambda_cv_moj=0.01, lambda_cv_ed=0.01):\n        super().__init__()\n        self.encoder_rgb = encoder_rgb\n        self.encoder_gray = encoder_gray\n        self.classifier = classifier\n        self.mode = mode\n        self.loss_ce = tf.keras.losses.CategoricalCrossentropy()\n        self.lambda_cv_moj = lambda_cv_moj\n        self.lambda_cv_ed = lambda_cv_ed\n\n        # metrics\n        self.train_acc = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n        self.val_acc   = tf.keras.metrics.CategoricalAccuracy(name=\"val_accuracy\")\n\n        self.cv_tracker = tf.keras.metrics.Mean(name=\"cv_loss\")\n\n    def call(self, inputs, training=False):\n        # For compatibility with model.fit API when using predict/evaluate\n        if self.mode == 'moj':\n            x1,x2,x3 = inputs\n            f1 = self.encoder_rgb(x1, training=training)\n            f2 = self.encoder_rgb(x2, training=training)\n            f3 = self.encoder_rgb(x3, training=training)\n            feat = (f1 + f2 + f3) / 3.0\n            return self.classifier(feat, training=training)\n        elif self.mode == 'ed':\n            x1,x2,x3 = inputs\n            f1 = self.encoder_gray(x1, training=training)\n            f2 = self.encoder_gray(x2, training=training)\n            f3 = self.encoder_gray(x3, training=training)\n            feat = (f1 + f2 + f3) / 3.0\n            return self.classifier(feat, training=training)\n        elif self.mode == 'fusion':\n            x1_m,x2_m,x3_m, x1_e,x2_e,x3_e = inputs\n            f1_m = self.encoder_rgb(x1_m, training=training)\n            f2_m = self.encoder_rgb(x2_m, training=training)\n            f3_m = self.encoder_rgb(x3_m, training=training)\n            f_m = (f1_m + f2_m + f3_m)/3.0\n\n            f1_e = self.encoder_gray(x1_e, training=training)\n            f2_e = self.encoder_gray(x2_e, training=training)\n            f3_e = self.encoder_gray(x3_e, training=training)\n            f_e = (f1_e + f2_e + f3_e)/3.0\n\n            fused = layers.Concatenate()([f_m, f_e])\n            return self.classifier(fused, training=training)\n        else:\n            raise ValueError(\"Unknown mode\")\n\n    def train_step(self, data):\n        x, y = data\n        # x shape depends on mode\n        with tf.GradientTape() as tape:\n            if self.mode == 'moj':\n                x1,x2,x3 = x\n                f1 = self.encoder_rgb(x1, training=True)\n                f2 = self.encoder_rgb(x2, training=True)\n                f3 = self.encoder_rgb(x3, training=True)\n                # l2-normalize features (stabilizes CV loss)\n                f1n = tf.math.l2_normalize(f1, axis=1)\n                f2n = tf.math.l2_normalize(f2, axis=1)\n                f3n = tf.math.l2_normalize(f3, axis=1)\n                feat = (f1 + f2 + f3)/3.0\n                logits = self.classifier(feat, training=True)\n\n                CE = self.loss_ce(y, logits)\n                CV = (tf.reduce_mean(tf.square(f1n - f2n)) +\n                      tf.reduce_mean(tf.square(f2n - f3n)) +\n                      tf.reduce_mean(tf.square(f1n - f3n)))\n                loss = CE + self.lambda_cv_moj * CV\n\n            elif self.mode == 'ed':\n                x1,x2,x3 = x\n                f1 = self.encoder_gray(x1, training=True)\n                f2 = self.encoder_gray(x2, training=True)\n                f3 = self.encoder_gray(x3, training=True)\n                f1n = tf.math.l2_normalize(f1, axis=1)\n                f2n = tf.math.l2_normalize(f2, axis=1)\n                f3n = tf.math.l2_normalize(f3, axis=1)\n                feat = (f1 + f2 + f3)/3.0\n                logits = self.classifier(feat, training=True)\n\n                CE = self.loss_ce(y, logits)\n                CV = (tf.reduce_mean(tf.square(f1n - f2n)) +\n                      tf.reduce_mean(tf.square(f2n - f3n)) +\n                      tf.reduce_mean(tf.square(f1n - f3n)))\n                loss = CE + self.lambda_cv_ed * CV\n\n            elif self.mode == 'fusion':\n                x1_m,x2_m,x3_m, x1_e,x2_e,x3_e = x\n                f1_m = self.encoder_rgb(x1_m, training=True)\n                f2_m = self.encoder_rgb(x2_m, training=True)\n                f3_m = self.encoder_rgb(x3_m, training=True)\n                f1_e = self.encoder_gray(x1_e, training=True)\n                f2_e = self.encoder_gray(x2_e, training=True)\n                f3_e = self.encoder_gray(x3_e, training=True)\n\n                # normalize\n                f1_mn = tf.math.l2_normalize(f1_m, axis=1)\n                f2_mn = tf.math.l2_normalize(f2_m, axis=1)\n                f3_mn = tf.math.l2_normalize(f3_m, axis=1)\n\n                f1_en = tf.math.l2_normalize(f1_e, axis=1)\n                f2_en = tf.math.l2_normalize(f2_e, axis=1)\n                f3_en = tf.math.l2_normalize(f3_e, axis=1)\n\n                f_m = (f1_m + f2_m + f3_m)/3.0\n                f_e = (f1_e + f2_e + f3_e)/3.0\n                fused = tf.concat([f_m, f_e], axis=1)\n                logits = self.classifier(fused, training=True)\n\n                CE = self.loss_ce(y, logits)\n                CV_m = (tf.reduce_mean(tf.square(f1_mn - f2_mn)) +\n                        tf.reduce_mean(tf.square(f2_mn - f3_mn)) +\n                        tf.reduce_mean(tf.square(f1_mn - f3_mn)))\n                CV_e = (tf.reduce_mean(tf.square(f1_en - f2_en)) +\n                        tf.reduce_mean(tf.square(f2_en - f3_en)) +\n                        tf.reduce_mean(tf.square(f1_en - f3_en)))\n                loss = CE + self.lambda_cv_moj * CV_m + self.lambda_cv_ed * CV_e\n                CV = CV_m + CV_e\n\n            else:\n                raise ValueError(\"Unknown mode\")\n\n        # gradients and apply\n        trainable_vars = self.trainable_variables\n        grads = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # update metrics\n        self.train_acc.update_state(y, logits)\n        # return values that Keras understands and which will be recorded in history\n        self.cv_tracker.update_state(CV)\n        return {\n            \"loss\": loss,\n            \"accuracy\": self.train_acc.result(),\n        }\n\n\n    def test_step(self, data):\n        x, y = data\n        if self.mode == 'moj':\n            x1,x2,x3 = x\n            f1 = self.encoder_rgb(x1, training=False)\n            f2 = self.encoder_rgb(x2, training=False)\n            f3 = self.encoder_rgb(x3, training=False)\n            feat = (f1 + f2 + f3)/3.0\n            logits = self.classifier(feat, training=False)\n        elif self.mode == 'ed':\n            x1,x2,x3 = x\n            f1 = self.encoder_gray(x1, training=False)\n            f2 = self.encoder_gray(x2, training=False)\n            f3 = self.encoder_gray(x3, training=False)\n            feat = (f1 + f2 + f3)/3.0\n            logits = self.classifier(feat, training=False)\n        elif self.mode == 'fusion':\n            x1_m,x2_m,x3_m, x1_e,x2_e,x3_e = x\n            f_m = (self.encoder_rgb(x1_m, training=False) + self.encoder_rgb(x2_m, training=False) + self.encoder_rgb(x3_m, training=False))/3.0\n            f_e = (self.encoder_gray(x1_e, training=False) + self.encoder_gray(x2_e, training=False) + self.encoder_gray(x3_e, training=False))/3.0\n            fused = tf.concat([f_m, f_e], axis=1)\n            logits = self.classifier(fused, training=False)\n        else:\n            raise ValueError(\"Unknown mode\")\n\n        CE = self.loss_ce(y, logits)\n        self.val_acc.update_state(y, logits)\n        return {\n            \"loss\": CE,                      \n            \"accuracy\": self.val_acc.result(),\n        }\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:34:53.643210Z","iopub.execute_input":"2025-11-25T07:34:53.643986Z","iopub.status.idle":"2025-11-25T07:34:53.664857Z","shell.execute_reply.started":"2025-11-25T07:34:53.643958Z","shell.execute_reply":"2025-11-25T07:34:53.664072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_callbacks():\n    return [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            mode='max',\n            patience=6,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3\n        )\n    ]\n\n\ndef plot_history(history):\n    hist = history.history\n\n    epochs = range(len(hist['loss']))\n\n    # Plot 1 — Loss\n    plt.figure(figsize=(12,5))\n    plt.plot(epochs, hist['loss'], label=\"Train Loss\")\n    plt.plot(epochs, hist['val_loss'], label=\"Val Loss\")\n    plt.title(\"Train vs Val Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # Plot 2 — Accuracy\n    plt.figure(figsize=(12,5))\n    plt.plot(epochs, hist['accuracy'], label=\"Train Acc\")\n    plt.plot(epochs, hist['val_accuracy'], label=\"Val Acc\")\n    plt.title(\"Train vs Val Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:34:56.267608Z","iopub.execute_input":"2025-11-25T07:34:56.267915Z","iopub.status.idle":"2025-11-25T07:34:56.274437Z","shell.execute_reply.started":"2025-11-25T07:34:56.267893Z","shell.execute_reply":"2025-11-25T07:34:56.273771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main_train(mode=MODE):\n    # collect files\n    moj_samples = collect_samples(MOJ_ROOT)\n    ed_mhi_samples = collect_samples(ED_MHI_ROOT)\n\n    moj_common = intersect_views(moj_samples)\n    ed_common = intersect_views(ed_mhi_samples)\n    all_common = moj_common & ed_common\n    print(\"Total common samples:\", len(all_common))\n\n    # cross-subject split as before (s01-s09 train, s10 test)\n    train_ids = [aid for aid in all_common if parse_subject_id(aid) in [f\"s{str(i).zfill(2)}\" for i in range(1,10)]]\n    test_ids = [aid for aid in all_common if parse_subject_id(aid) == \"s10\"]\n    print(\"Train samples:\", len(train_ids), \"Test samples:\", len(test_ids))\n\n    # build triplet datasets\n    (X1_moj, X2_moj, X3_moj,\n     X1_ed, X2_ed, X3_ed,\n     Y, le, num_classes) = build_triplet_dataset(moj_samples, ed_mhi_samples, train_ids, target_size=TARGET_SIZE)\n\n    (X1_moj_test, X2_moj_test, X3_moj_test,\n     X1_ed_test, X2_ed_test, X3_ed_test,\n     Y_test, _, _) = build_triplet_dataset(moj_samples, ed_mhi_samples, test_ids, target_size=TARGET_SIZE, le=le)\n\n    print(\"num_classes:\", num_classes)\n\n    # build encoders/classifier fresh (IMPORTANT to reinitialize weights between runs)\n    encoder_rgb = build_encoder_rgb(input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3), use_augment=True)\n    encoder_gray = build_encoder_gray(input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 1), use_augment=True)\n    # classifier = build_classifier(num_classes)\n\n    # MODE-specific model\n    if mode == 'moj':\n        classifier = build_classifier(num_classes, input_dim=256)\n        model = CrossViewModel(encoder_rgb=encoder_rgb, classifier=classifier, mode='moj', lambda_cv_moj=LAMBDA_CV_MOJ)\n        train_inputs = [X1_moj, X2_moj, X3_moj]\n        val_inputs = [X1_moj_test, X2_moj_test, X3_moj_test]\n\n    elif mode == 'ed':\n        classifier = build_classifier(num_classes, input_dim=256)\n        model = CrossViewModel(encoder_gray=encoder_gray, classifier=classifier, mode='ed', lambda_cv_ed=LAMBDA_CV_ED)\n        train_inputs = [X1_ed, X2_ed, X3_ed]\n        val_inputs = [X1_ed_test, X2_ed_test, X3_ed_test]\n        \n\n    elif mode == 'fusion':\n        classifier = build_classifier(num_classes, input_dim=512)\n        model = CrossViewModel(encoder_rgb=encoder_rgb, encoder_gray=encoder_gray, classifier=classifier,\n                               mode='fusion', lambda_cv_moj=LAMBDA_CV_MOJ, lambda_cv_ed=LAMBDA_CV_ED)\n        train_inputs = [X1_moj, X2_moj, X3_moj, X1_ed, X2_ed, X3_ed]\n        val_inputs = [X1_moj_test, X2_moj_test, X3_moj_test, X1_ed_test, X2_ed_test, X3_ed_test]\n        \n    \n    else:\n        raise ValueError(\"Unknown mode\")\n       \n    print(\"\\n====== ENCODER RGB SUMMARY ======\")\n    encoder_rgb.summary()\n\n    print(\"\\n====== ENCODER GRAY SUMMARY ======\")\n    encoder_gray.summary()\n\n    print(\"\\n====== CLASSIFIER SUMMARY ======\")\n    classifier.summary()\n\n    print(model.summary())\n    \n    # compile and callbacks\n    model.compile(optimizer=tf.keras.optimizers.Adam(LR_HEAD))\n    # callbacks = make_callbacks()\n\n    # stage 1: freeze encoders (warm-up)\n    if mode in ('moj','fusion'):\n        encoder_rgb.trainablkwoduse = False\n    if mode in ('ed','fusion'):\n        encoder_gray.trainable = False\n    model.compile(optimizer=tf.keras.optimizers.Adam(LR_HEAD))\n    print(\"Stage 1 (warm-up) training with encoder frozen...\")\n    history1 = model.fit(\n        train_inputs, Y,\n        validation_data=(val_inputs, Y_test),\n        epochs=EPOCHS_HEAD, batch_size=BATCH_SIZE, verbose=1\n    )\n\n    # stage 2: unfreeze and fine-tune\n    if mode in ('moj','fusion'):\n        encoder_rgb.trainable = True\n    if mode in ('ed','fusion'):\n        encoder_gray.trainable = True\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(LR_FINETUNE))\n    print(\"Stage 2 (fine-tune) training with encoder unfrozen...\")\n    history2 = model.fit(\n        train_inputs, Y,\n        validation_data=(val_inputs, Y_test),\n        initial_epoch=history1.epoch[-1] if hasattr(history1,'epoch') else 0,\n        epochs=history1.epoch[-1] + EPOCHS_FINETUNE + 1 if hasattr(history1,'epoch') else EPOCHS_FINETUNE,\n        batch_size=BATCH_SIZE, verbose=1\n    )\n\n    # combine histories for plotting convenience (simple concat)\n    merged_history = {}\n    for k in set(list(history1.history.keys()) + list(history2.history.keys())):\n        merged_history[k] = history1.history.get(k, []) + history2.history.get(k, [])\n    merged = type(\"H\", (), {\"history\": merged_history})\n\n    # evaluate\n    eval_results = model.evaluate(val_inputs, Y_test, verbose=0)\n    print(\"Final eval (on test set):\", eval_results)\n\n    # plots\n    plot_history(merged)\n\n    # save final model weights\n    try:\n        model.save_weights(MODEL_SAVE_PATH)\n        print(\"Saved weights to\", MODEL_SAVE_PATH)\n    except Exception as e:\n        print(\"Could not save weights:\", e)\n\n        # ---- CONFUSION MATRIX ----\n    from sklearn.metrics import confusion_matrix\n    import numpy as np\n\n    # get predictions\n    y_pred = model.predict(val_inputs)\n    y_pred = np.argmax(y_pred, axis=1)\n    y_true = np.argmax(Y_test, axis=1)\n\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    plt.imshow(cm)\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    \n    # Add numbers inside the boxes\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, cm[i, j], ha='center', va='center')\n    \n    plt.colorbar()\n    plt.tight_layout()\n    plt.show()\n\n    return model, merged, le\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:54:44.953781Z","iopub.execute_input":"2025-11-25T07:54:44.954463Z","iopub.status.idle":"2025-11-25T07:54:44.971106Z","shell.execute_reply.started":"2025-11-25T07:54:44.954440Z","shell.execute_reply":"2025-11-25T07:54:44.970375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    model, history, le = main_train(mode='moj')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:55:20.968693Z","iopub.execute_input":"2025-11-25T07:55:20.969555Z","iopub.status.idle":"2025-11-25T08:01:18.215916Z","shell.execute_reply.started":"2025-11-25T07:55:20.969522Z","shell.execute_reply":"2025-11-25T08:01:18.215218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    model, history, le = main_train(mode='ed')\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T08:58:06.403044Z","iopub.execute_input":"2025-11-25T08:58:06.403347Z","iopub.status.idle":"2025-11-25T09:04:04.829916Z","shell.execute_reply.started":"2025-11-25T08:58:06.403326Z","shell.execute_reply":"2025-11-25T09:04:04.829167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    model, history, le = main_train(mode='fusion')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T08:26:56.542997Z","iopub.execute_input":"2025-11-25T08:26:56.543601Z","iopub.status.idle":"2025-11-25T08:41:00.593262Z","shell.execute_reply.started":"2025-11-25T08:26:56.543575Z","shell.execute_reply":"2025-11-25T08:41:00.592501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}