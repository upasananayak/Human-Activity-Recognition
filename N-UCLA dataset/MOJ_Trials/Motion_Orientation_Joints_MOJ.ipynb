{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5225858,"sourceType":"datasetVersion","datasetId":3040488}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kwodus/moj-nucla?scriptVersionId=282397314\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Averaging","metadata":{}},{"cell_type":"markdown","source":"### Averaging on one image","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n\ndef arccos_safe(x):\n    return np.arccos(np.clip(x, -1.0, 1.0))\n\ndef normalize_to_rgb(coords):\n    min_val = np.min(coords)\n    max_val = np.max(coords)\n    norm_coords = 255 * (coords - min_val) / (max_val - min_val + 1e-8)\n    return norm_coords.astype(np.uint8)\n\ndef compute_motion_of_joints(skeleton_sequence):\n    Sx = normalize_to_rgb(skeleton_sequence[:, :, 0])\n    Sy = normalize_to_rgb(skeleton_sequence[:, :, 1])\n    Sz = normalize_to_rgb(skeleton_sequence[:, :, 2])\n    motion = np.stack([Sx, Sy, Sz], axis=2)   # (N,J,3)\n    return motion\n\ndef compute_pose_orientation(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    pose_orientation = np.zeros((N, J, 3))\n    for t in range(N):\n        for i in range(J):\n            for j in range(J):\n                if i == j:\n                    continue\n                diff = skeleton_sequence[t, i] - skeleton_sequence[t, j]\n                norm = np.linalg.norm(diff) + 1e-8\n                normalized = diff / norm\n                pose_orientation[t, i] += arccos_safe(normalized)\n            pose_orientation[t, i] /= (J - 1)\n    return normalize_to_rgb(pose_orientation)\n\ndef compute_transition_orientation(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    transition_orientation = np.zeros((N, J, 3))\n    for t in range(1, N):\n        for i in range(J):\n            for j in range(J):\n                if i == j:\n                    continue\n                diff = skeleton_sequence[t, i] - skeleton_sequence[t-1, j]\n                norm = np.linalg.norm(diff) + 1e-8\n                normalized = diff / norm\n                transition_orientation[t, i] += arccos_safe(normalized)\n            transition_orientation[t, i] /= (J - 1)\n    return normalize_to_rgb(transition_orientation)\n\ndef build_moj_image_side_by_side(motion, pose, transition):\n    N, J, _ = motion.shape\n\n    pose_resized = cv2.resize(pose, (J, N), interpolation=cv2.INTER_CUBIC)\n    transition_resized = cv2.resize(transition, (J, N), interpolation=cv2.INTER_CUBIC)\n\n    PT_side = np.hstack([pose_resized, transition_resized])\n\n    combined = np.hstack([motion, PT_side]).astype(np.float32)\n\n    mn, mx = combined.min(), combined.max()\n    if mx - mn < 1e-8:\n        norm_img = np.zeros_like(combined, dtype=np.uint8)\n    else:\n        norm_img = 255.0 * (combined - mn) / (mx - mn)\n        norm_img = np.clip(norm_img, 0, 255).astype(np.uint8)\n\n    final = cv2.resize(norm_img, (224, 224), interpolation=cv2.INTER_CUBIC)\n    return final\n\n\n\ndef test_one_sequence(seq_path, expected_joints=20):\n    \n    def load_skeleton_file(filepath):\n        joints = []\n        with open(filepath, 'r') as f:\n            lines = f.read().strip().split('\\n')\n            for line in lines[1:]:\n                if not line.strip(): continue\n                parts = line.strip().split(',')\n                if len(parts) < 3: continue\n                x, y, z = float(parts[0]), float(parts[1]), float(parts[2])\n                joints.append([x, y, z])\n        return np.array(joints, dtype=np.float32)\n\n    def load_sequence_skeletons(sequence_path, expected_joints):\n        file_list_path = os.path.join(sequence_path, 'fileList.txt')\n        frames = []\n        with open(file_list_path, 'r') as f:\n            for line in f:\n                frame_id = line.strip().split()[0]\n                skeleton_file = None\n                for file in os.listdir(sequence_path):\n                    if file.startswith(f\"frame_{frame_id}_\") and file.endswith(\"_skeletons.txt\"):\n                        skeleton_file = file\n                        break\n                if skeleton_file is None: continue\n                joints = load_skeleton_file(os.path.join(sequence_path, skeleton_file))\n                if joints.shape[0] == expected_joints:\n                    frames.append(joints)\n        return np.stack(frames) if frames else None\n\n    skeleton_sequence = load_sequence_skeletons(seq_path, expected_joints)\n    if skeleton_sequence is None:\n        print(\"No skeletons found.\")\n        return\n\n    motion = compute_motion_of_joints(skeleton_sequence)\n    pose = compute_pose_orientation(skeleton_sequence)\n    transition = compute_transition_orientation(skeleton_sequence)\n    moj_image = build_moj_image_side_by_side(motion, pose, transition)\n\n\n    print(\"Motion shape:\", motion.shape)\n    print(\"Pose shape:\", pose.shape)\n    print(\"Transition shape:\", transition.shape)\n    print(\"Final MOJ image shape:\", moj_image.shape)\n\n    fig, axes = plt.subplots(1, 4, figsize=(20,5))\n    axes[0].imshow(motion); axes[0].set_title(\"Motion (S)\"); axes[0].axis('off')\n    axes[1].imshow(pose); axes[1].set_title(\"Pose (P)\"); axes[1].axis('off')\n    axes[2].imshow(transition); axes[2].set_title(\"Transition (T)\"); axes[2].axis('off')\n    axes[3].imshow(moj_image); axes[3].set_title(\"Final MOJ Image I=[S,[P,T]]\"); axes[3].axis('off')\n    plt.show()\n\nseq_path = \"/kaggle/input/n-ucla/multiview_action/view_1/a01_s01_e01\"\ntest_one_sequence(seq_path, expected_joints=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T04:23:31.485979Z","iopub.execute_input":"2025-11-25T04:23:31.486268Z","iopub.status.idle":"2025-11-25T04:23:33.491205Z","shell.execute_reply.started":"2025-11-25T04:23:31.486249Z","shell.execute_reply":"2025-11-25T04:23:33.490498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Averaging on all images","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\n\n\ndef arccos_safe(x):\n    return np.arccos(np.clip(x, -1.0, 1.0))\n\ndef normalize_to_rgb(coords):\n    min_val = np.min(coords)\n    max_val = np.max(coords)\n    norm_coords = 255 * (coords - min_val) / (max_val - min_val + 1e-8)\n    return norm_coords.astype(np.uint8)\n\ndef compute_motion_of_joints(skeleton_sequence):\n    Sx = normalize_to_rgb(skeleton_sequence[:, :, 0])\n    Sy = normalize_to_rgb(skeleton_sequence[:, :, 1])\n    Sz = normalize_to_rgb(skeleton_sequence[:, :, 2])\n    motion = np.stack([Sx, Sy, Sz], axis=2)   # (N,J,3)\n    return motion\n\ndef compute_pose_orientation(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    pose_orientation = np.zeros((N, J, 3))\n    for t in range(N):\n        for i in range(J):\n            for j in range(J):\n                if i == j:\n                    continue\n                diff = skeleton_sequence[t, i] - skeleton_sequence[t, j]\n                norm = np.linalg.norm(diff) + 1e-8\n                normalized = diff / norm\n                pose_orientation[t, i] += arccos_safe(normalized)\n            pose_orientation[t, i] /= (J - 1)\n    return normalize_to_rgb(pose_orientation)\n\n\ndef compute_transition_orientation(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    transition_orientation = np.zeros((N, J, 3))\n    for t in range(1, N):\n        for i in range(J):\n            for j in range(J):\n                if i == j:\n                    continue\n                diff = skeleton_sequence[t, i] - skeleton_sequence[t-1, j]\n                norm = np.linalg.norm(diff) + 1e-8\n                normalized = diff / norm\n                transition_orientation[t, i] += arccos_safe(normalized)\n            transition_orientation[t, i] /= (J - 1)\n    return normalize_to_rgb(transition_orientation)\n\ndef build_moj_image_side_by_side(motion, pose, transition):\n    N, J, _ = motion.shape\n\n    pose_resized = cv2.resize(pose, (J, N), interpolation=cv2.INTER_CUBIC)\n    transition_resized = cv2.resize(transition, (J, N), interpolation=cv2.INTER_CUBIC)\n\n    PT_side = np.hstack([pose_resized, transition_resized])\n\n    combined = np.hstack([motion, PT_side]).astype(np.float32)\n\n    mn, mx = combined.min(), combined.max()\n    if mx - mn < 1e-8:\n        norm_img = np.zeros_like(combined, dtype=np.uint8)\n    else:\n        norm_img = 255.0 * (combined - mn) / (mx - mn)\n        norm_img = np.clip(norm_img, 0, 255).astype(np.uint8)\n\n    \n    final = cv2.resize(norm_img, (224, 224), interpolation=cv2.INTER_CUBIC)\n    return final\n\ndef load_skeleton_file(filepath):\n    joints = []\n    with open(filepath, 'r') as f:\n        lines = f.read().strip().split('\\n')\n        for line in lines[1:]:\n            if not line.strip(): continue\n            parts = line.strip().split(',')\n            if len(parts) < 3: continue\n            x, y, z = float(parts[0]), float(parts[1]), float(parts[2])\n            joints.append([x, y, z])\n    return np.array(joints, dtype=np.float32)\n\ndef load_sequence_skeletons(sequence_path, expected_joints=20):\n    file_list_path = os.path.join(sequence_path, 'fileList.txt')\n    frames = []\n    with open(file_list_path, 'r') as f:\n        for line in f:\n            frame_id = line.strip().split()[0]\n            skeleton_file = None\n            for file in os.listdir(sequence_path):\n                if file.startswith(f\"frame_{frame_id}_\") and file.endswith(\"_skeletons.txt\"):\n                    skeleton_file = file\n                    break\n            if skeleton_file is None: continue\n            joints = load_skeleton_file(os.path.join(sequence_path, skeleton_file))\n            if joints.shape[0] == expected_joints:\n                frames.append(joints)\n    return np.stack(frames) if frames else None\n\ndef process_nucla_dataset(base_path, output_dir):\n    views_dir = os.path.join(base_path, 'multiview_action')\n    os.makedirs(output_dir, exist_ok=True)\n\n    moj_image_paths = []\n    view_folders = [v for v in sorted(os.listdir(views_dir)) if os.path.isdir(os.path.join(views_dir, v))]\n\n    for view_folder in view_folders:\n        view_path = os.path.join(views_dir, view_folder)\n        seq_folders = [s for s in sorted(os.listdir(view_path)) if os.path.isdir(os.path.join(view_path, s))]\n        \n        print(f\"Processing view: {view_folder} ({len(seq_folders)} sequences)\")\n        for seq_folder in tqdm(seq_folders, desc=f\"{view_folder} sequences\"):\n            seq_path = os.path.join(view_path, seq_folder)\n            skeleton_sequence = load_sequence_skeletons(seq_path)\n            if skeleton_sequence is None:\n                continue\n\n            motion = compute_motion_of_joints(skeleton_sequence)\n            pose = compute_pose_orientation(skeleton_sequence)\n            transition = compute_transition_orientation(skeleton_sequence)\n            moj_image = build_moj_image_side_by_side(motion, pose, transition)\n\n            save_path = os.path.join(output_dir, f\"{view_folder}_{seq_folder}.png\")\n            cv2.imwrite(save_path, cv2.cvtColor(moj_image, cv2.COLOR_RGB2BGR))\n            moj_image_paths.append(save_path)\n    return moj_image_paths\n\ndef visualize_moj_images(image_paths, num_images=5):\n    selected_paths = random.sample(image_paths, min(len(image_paths), num_images))\n    plt.figure(figsize=(15, 3 * num_images))\n    for i, path in enumerate(selected_paths):\n        img = cv2.imread(path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(num_images, 1, i + 1)\n        plt.imshow(img_rgb)\n        plt.title(os.path.basename(path))\n        plt.axis('off')\n    plt.show()\n\nbase_path = '/kaggle/input/n-ucla'\noutput_moj_dir = '/kaggle/working/moj_images'\n\nmoj_image_paths = process_nucla_dataset(base_path, output_moj_dir)\nvisualize_moj_images(moj_image_paths, num_images=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T04:30:07.916311Z","iopub.execute_input":"2025-11-25T04:30:07.916609Z","iopub.status.idle":"2025-11-25T04:48:34.430835Z","shell.execute_reply.started":"2025-11-25T04:30:07.916586Z","shell.execute_reply":"2025-11-25T04:48:34.430055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"markdown","source":"#### Small Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, regularizers\n\ndef build_moj_cnn(input_shape=(224, 224, 3), num_classes=10, lr=0.0005):\n\n    model = models.Sequential()\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\",\n                            input_shape=input_shape,\n                            kernel_initializer=\"he_normal\",\n                            kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\",\n                            kernel_initializer=\"he_normal\",\n                            kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n\n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(256, kernel_initializer=\"he_normal\"))\n    model.add(layers.ReLU())\n    model.add(layers.Dropout(0.5))\n\n    model.add(layers.Dense(128, kernel_initializer=\"he_normal\"))\n    model.add(layers.ReLU())\n    model.add(layers.Dropout(0.5))\n\n\n    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n\n    optimizer = optimizers.Adam(learning_rate=lr)\n\n    model.compile(optimizer=optimizer,\n                  loss=\"categorical_crossentropy\",\n                  metrics=[\"accuracy\"])\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T04:50:47.954959Z","iopub.execute_input":"2025-11-25T04:50:47.955271Z","iopub.status.idle":"2025-11-25T04:51:07.206779Z","shell.execute_reply.started":"2025-11-25T04:50:47.955246Z","shell.execute_reply":"2025-11-25T04:51:07.205732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train and test using normal train test split","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\n\ndef load_moj_dataset(base_dir, target_size=(224,224)):\n    X, y = [], []\n    for fname in tqdm(os.listdir(base_dir), desc=\"Loading MOJ images\"):\n        if not fname.endswith(\".png\"):\n            continue\n        \n        parts = fname.split(\"_\")\n        if len(parts) < 2:\n            continue\n        action_label = parts[1]   # \"a01\", \"a02\", ...\n\n        img_path = os.path.join(base_dir, fname)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, target_size)\n\n        X.append(img)\n        y.append(action_label)\n    \n    return np.array(X, dtype=np.float32), np.array(y)\n\n\ndef train_on_nucla(moj_dir, num_classes=10, batch_size=64, epochs=30):\n    \n    X, y = load_moj_dataset(moj_dir, target_size=(224,224))\n    X = X / 255.0  \n\n    le = LabelEncoder()\n    y_int = le.fit_transform(y)\n    y_cat = to_categorical(y_int, num_classes=num_classes)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n    )\n\n    model= build_moj_cnn(input_shape=(224,224,3), num_classes=num_classes)\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=10,\n        batch_size=batch_size,\n        verbose=1\n    )\n\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\" Test Accuracy: {test_acc:.4f}\")\n\n    return model, history, le, x_test, y_test\n\n\n\nmoj_dir = \"/kaggle/working/moj_images\"  \nmodel, history, label_encoder, x_test, y_test = train_on_nucla(moj_dir, num_classes=10, epochs=30)\nmodel.summary()\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\ncm = confusion_matrix(y_true, y_pred)\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=label_encoder.classes_,\n            yticklabels=label_encoder.classes_)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix - MOJ CNN\")\nplt.show()\n\n\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:02:20.82979Z","iopub.execute_input":"2025-11-25T05:02:20.830323Z","iopub.status.idle":"2025-11-25T05:03:21.403096Z","shell.execute_reply.started":"2025-11-25T05:02:20.830298Z","shell.execute_reply":"2025-11-25T05:03:21.402338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train and test using Leave one subject out (LOSO) training and testing","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\n\n\nIMG_DIR = \"/kaggle/working/moj_images\"\nIMG_SIZE = (224, 224)\nNUM_CLASSES = 10   # number of action classes in dataset\nEPOCHS = 10\nBATCH_SIZE = 32\n\n\ndef parse_filename(filename):\n    \"\"\"\n    Extract view, action, subject, execution from filename\n    Example: view_1_a03_s10_e03.png\n    \"\"\"\n    match = re.match(r\"view_(\\d+)_a(\\d+)_s(\\d+)_e(\\d+)\\.png\", filename)\n    if match:\n        view, action, subject, execution = map(int, match.groups())\n        return view, action, subject, execution\n    return None\n\ndef build_action_map():\n    \"\"\"Build a mapping from actual action numbers to continuous 0..NUM_CLASSES-1\"\"\"\n    action_ids = set()\n    for fname in os.listdir(IMG_DIR):\n        parsed = parse_filename(fname)\n        if parsed:\n            _, action, _, _ = parsed\n            action_ids.add(action)\n    action_ids = sorted(list(action_ids))\n    action_map = {aid: idx for idx, aid in enumerate(action_ids)}\n    return action_map\n\ndef load_dataset(img_dir=IMG_DIR, img_size=IMG_SIZE, action_map=None):\n    X, y, subjects = [], [], []\n    for fname in os.listdir(img_dir):\n        if not fname.endswith(\".png\"):\n            continue\n        parsed = parse_filename(fname)\n        if not parsed:\n            continue\n        view, action, subject, execution = parsed\n        img_path = os.path.join(img_dir, fname)\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, img_size)\n        img = img / 255.0\n        X.append(img)\n        y.append(action_map[action])  # map to 0..NUM_CLASSES-1\n        subjects.append(subject)\n    return np.array(X), np.array(y), np.array(subjects)\n\n\ndef run_single_subject_experiment(test_subject=1):\n    # Build action mapping\n    action_map = build_action_map()\n    print(\"Action mapping:\", action_map)\n\n    X, y, subjects = load_dataset(action_map=action_map)\n\n    \n    train_idx = subjects != test_subject\n    test_idx = subjects == test_subject\n\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n    y_train = to_categorical(y_train, NUM_CLASSES)\n    y_test = to_categorical(y_test, NUM_CLASSES)\n\n    model = build_moj_cnn(input_shape=(224,224,3), num_classes=NUM_CLASSES)\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        verbose=2\n    )\n\n    from sklearn.metrics import confusion_matrix, classification_report\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n\n    y_pred_probs = model.predict(X_test)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n\n    cm = confusion_matrix(y_true, y_pred)\n\n    plt.figure(figsize=(10,8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix for subject {test_subject}\")\n    plt.show()\n\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred))\n\n\n    print(model.summary())\n    \n    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"\\nFinal Accuracy for subject {test_subject}: {acc:.4f}\")\n    return acc\n\n\nacc = run_single_subject_experiment(test_subject=5)   \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:07:36.011325Z","iopub.execute_input":"2025-11-25T05:07:36.011643Z","iopub.status.idle":"2025-11-25T05:08:50.628298Z","shell.execute_reply.started":"2025-11-25T05:07:36.011621Z","shell.execute_reply":"2025-11-25T05:08:50.627577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## With PoT2i","metadata":{}},{"cell_type":"markdown","source":"### Full dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\n\n\ndef arccos_safe(x):\n    return np.arccos(np.clip(x, -1.0, 1.0))\n\ndef normalize_to_rgb(coords):\n    min_val = np.min(coords)\n    max_val = np.max(coords)\n    if max_val - min_val < 1e-8:\n        return np.zeros_like(coords, dtype=np.uint8)\n    norm_coords = 255.0 * (coords - min_val) / (max_val - min_val + 1e-8)\n    return np.clip(norm_coords, 0, 255).astype(np.uint8)\n\ndef normalize_to_0_255(arr):\n    a = arr.astype(np.float32)\n    mn, mx = a.min(), a.max()\n    if mx - mn < 1e-8:\n        return np.zeros_like(a, dtype=np.uint8)\n    scaled = 255.0 * (a - mn) / (mx - mn)\n    return np.clip(scaled, 0, 255).astype(np.uint8)\n\n\ndef compute_motion_of_joints(skeleton_sequence):\n    Sx = normalize_to_rgb(skeleton_sequence[:, :, 0])\n    Sy = normalize_to_rgb(skeleton_sequence[:, :, 1])\n    Sz = normalize_to_rgb(skeleton_sequence[:, :, 2])\n    motion = np.stack([Sx, Sy, Sz], axis=2)  # (N,J,3)\n    return motion\n\n\ndef get_pair_indices(J):\n    return [(i, j) for i in range(J) for j in range(J) if i != j]\n\ndef compute_pairwise_pot2i_features(skeleton_sequence):\n    \"\"\"\n    Returns:\n      combined: (N, 4*M_pairs, 3) uint8\n    \"\"\"\n    N, J, _ = skeleton_sequence.shape\n    pairs = get_pair_indices(J)\n    M = len(pairs)\n\n    # Axes\n    Ox = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n    Oy = np.array([0.0, 1.0, 0.0], dtype=np.float32)\n    Oz = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n\n    # Containers\n    pose_dists = np.zeros((N, M, 3), dtype=np.float32)\n    pose_orients = np.zeros((N, M, 3), dtype=np.float32)\n    trans_dists = np.zeros((N, M, 3), dtype=np.float32)\n    trans_orients = np.zeros((N, M, 3), dtype=np.float32)\n\n    for t in range(N):\n        for idx, (i, j) in enumerate(pairs):\n            # Pose vector\n            u = skeleton_sequence[t, i] - skeleton_sequence[t, j]\n            pose_dists[t, idx, :] = np.abs(u)  # projected distances δx,δy,δz\n            norm_u = np.linalg.norm(u) + 1e-8\n            u_unit = u / norm_u\n            pose_orients[t, idx, 0] = arccos_safe(np.dot(u_unit, Ox))\n            pose_orients[t, idx, 1] = arccos_safe(np.dot(u_unit, Oy))\n            pose_orients[t, idx, 2] = arccos_safe(np.dot(u_unit, Oz))\n\n            # Transition vector\n            if t == 0:\n                trans_dists[t, idx, :] = 0.0\n                trans_orients[t, idx, :] = 0.0\n            else:\n                v = skeleton_sequence[t, i] - skeleton_sequence[t-1, j]\n                trans_dists[t, idx, :] = np.abs(v)  # λx,λy,λz\n                norm_v = np.linalg.norm(v) + 1e-8\n                v_unit = v / norm_v\n                trans_orients[t, idx, 0] = arccos_safe(np.dot(v_unit, Ox))\n                trans_orients[t, idx, 1] = arccos_safe(np.dot(v_unit, Oy))\n                trans_orients[t, idx, 2] = arccos_safe(np.dot(v_unit, Oz))\n\n    # Normalize separately as per PoT2I\n    pose_dists = normalize_to_0_255(pose_dists)\n    trans_dists = normalize_to_0_255(trans_dists)\n    pose_orients = normalize_to_0_255(pose_orients)\n    trans_orients = normalize_to_0_255(trans_orients)\n\n    # Arrange into pixels: [pose_dist, pose_orient, trans_dist, trans_orient] → 4*M width\n    combined = np.concatenate([pose_dists, pose_orients, trans_dists, trans_orients], axis=0)  # (N, 4*M, 3)\n    return combined\n\ndef make_PoT2I_image(skeleton_sequence, target_height, target_width):\n    full = compute_pairwise_pot2i_features(skeleton_sequence)  # (4N, M, 3)\n    H, W, _ = full.shape\n    resized = cv2.resize(full, (target_width, target_height), interpolation=cv2.INTER_CUBIC)\n    if resized.dtype != np.uint8:\n        resized = np.clip(resized, 0, 255).astype(np.uint8)\n    return resized  # (target_height, target_width, 3)\n\n\ndef concatenate_motion_and_pot(motion, pot_resized):\n    N, J, _ = motion.shape\n    # Ensure same height\n    H = motion.shape[0]\n    W = motion.shape[1]\n\n    # Resize pot to match motion height\n    pot_resized = cv2.resize(pot_resized, (W, H), interpolation=cv2.INTER_CUBIC)\n\n    # Concatenate horizontally (left-right)\n    combined = np.concatenate([motion, pot_resized], axis=1)  # shape (H, 2*W, 3)\n\n    # Normalize to [0,255]\n    mn, mx = combined.min(), combined.max()\n    if mx - mn < 1e-8:\n        norm_img = np.zeros_like(combined, dtype=np.uint8)\n    else:\n        norm_img = 255.0 * (combined - mn) / (mx - mn)\n        norm_img = np.clip(norm_img, 0, 255).astype(np.uint8)\n\n    # Resize final to 224x224\n    final = cv2.resize(norm_img, (224, 224), interpolation=cv2.INTER_CUBIC)\n    return final\n\n\ndef load_skeleton_file(filepath):\n    joints = []\n    with open(filepath, 'r') as f:\n        lines = f.read().strip().split('\\n')\n        for line in lines[1:]:\n            if not line.strip():\n                continue\n            parts = line.strip().split(',')\n            if len(parts) < 3:\n                continue\n            x, y, z = float(parts[0]), float(parts[1]), float(parts[2])\n            joints.append([x, y, z])\n    return np.array(joints, dtype=np.float32)\n\ndef compute_pot2i_blocks(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    pairs = get_pair_indices(J)\n    M = len(pairs)\n\n    Ox, Oy, Oz = np.eye(3, dtype=np.float32)\n\n    pose_dists = np.zeros((N, M, 3), dtype=np.float32)\n    pose_orients = np.zeros((N, M, 3), dtype=np.float32)\n    trans_dists = np.zeros((N, M, 3), dtype=np.float32)\n    trans_orients = np.zeros((N, M, 3), dtype=np.float32)\n\n    for t in range(N):\n        for idx, (i, j) in enumerate(pairs):\n            u = skeleton_sequence[t, i] - skeleton_sequence[t, j]\n            pose_dists[t, idx, :] = np.abs(u)\n            norm_u = np.linalg.norm(u) + 1e-8\n            u_unit = u / norm_u\n            pose_orients[t, idx, :] = [arccos_safe(np.dot(u_unit, Ox)),\n                                       arccos_safe(np.dot(u_unit, Oy)),\n                                       arccos_safe(np.dot(u_unit, Oz))]\n            if t > 0:\n                v = skeleton_sequence[t, i] - skeleton_sequence[t-1, j]\n                trans_dists[t, idx, :] = np.abs(v)\n                norm_v = np.linalg.norm(v) + 1e-8\n                v_unit = v / norm_v\n                trans_orients[t, idx, :] = [arccos_safe(np.dot(v_unit, Ox)),\n                                            arccos_safe(np.dot(v_unit, Oy)),\n                                            arccos_safe(np.dot(v_unit, Oz))]\n\n    # Normalize\n    pose_dists = normalize_to_0_255(pose_dists)\n    pose_orients = normalize_to_0_255(pose_orients)\n    trans_dists = normalize_to_0_255(trans_dists)\n    trans_orients = normalize_to_0_255(trans_orients)\n\n    return pose_dists, pose_orients, trans_dists, trans_orients\n\n\ndef load_sequence_skeletons(sequence_path, expected_joints=20):\n    file_list_path = os.path.join(sequence_path, 'fileList.txt')\n    frames = []\n    if not os.path.exists(file_list_path):\n        return None\n    with open(file_list_path, 'r') as f:\n        for line in f:\n            frame_id = line.strip().split()[0]\n            skeleton_file = None\n            for file in os.listdir(sequence_path):\n                if file.startswith(f\"frame_{frame_id}_\") and file.endswith(\"_skeletons.txt\"):\n                    skeleton_file = file\n                    break\n            if skeleton_file is None:\n                continue\n            joints = load_skeleton_file(os.path.join(sequence_path, skeleton_file))\n            if joints.shape[0] == expected_joints:\n                frames.append(joints)\n    if not frames:\n        return None\n    return np.stack(frames)\n\n\ndef process_nucla_dataset(base_path, output_dir, expected_joints=20):\n    views_dir = os.path.join(base_path, 'multiview_action')\n    os.makedirs(output_dir, exist_ok=True)\n    moj_image_paths = []\n\n    view_folders = [v for v in sorted(os.listdir(views_dir)) if os.path.isdir(os.path.join(views_dir, v))]\n    for view_folder in view_folders:\n        view_path = os.path.join(views_dir, view_folder)\n        seq_folders = [s for s in sorted(os.listdir(view_path)) if os.path.isdir(os.path.join(view_path, s))]\n\n        print(f\"Processing view: {view_folder} ({len(seq_folders)} sequences)\")\n        for seq_folder in tqdm(seq_folders, desc=f\"{view_folder} sequences\"):\n            seq_path = os.path.join(view_path, seq_folder)\n            skeleton_sequence = load_sequence_skeletons(seq_path, expected_joints=expected_joints)\n            if skeleton_sequence is None:\n                continue\n\n            motion = compute_motion_of_joints(skeleton_sequence)\n            pot_resized = make_PoT2I_image(\n                skeleton_sequence,\n                target_height=motion.shape[0],  # match motion height (N)\n                target_width=motion.shape[1]    # match motion width (J)\n            )\n            moj_image = concatenate_motion_and_pot(motion, pot_resized)\n\n\n            save_path = os.path.join(output_dir, f\"{view_folder}_{seq_folder}.png\")\n            cv2.imwrite(save_path, cv2.cvtColor(moj_image, cv2.COLOR_RGB2BGR))\n            moj_image_paths.append(save_path)\n\n    return moj_image_paths\n\n\ndef visualize_moj_images(image_paths, num_images=5):\n    selected_paths = random.sample(image_paths, min(len(image_paths), num_images))\n    plt.figure(figsize=(15, 3 * num_images))\n    for i, path in enumerate(selected_paths):\n        img = cv2.imread(path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(num_images, 1, i + 1)\n        plt.imshow(img_rgb)\n        plt.title(os.path.basename(path))\n        plt.axis('off')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    base_path = '/kaggle/input/n-ucla'\n    output_moj_dir = '/kaggle/working/moj_images'\n    moj_image_paths = process_nucla_dataset(base_path, output_moj_dir, expected_joints=20)\n    if moj_image_paths:\n        visualize_moj_images(moj_image_paths, num_images=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:10:27.42645Z","iopub.execute_input":"2025-11-25T05:10:27.426942Z","iopub.status.idle":"2025-11-25T05:35:04.277758Z","shell.execute_reply.started":"2025-11-25T05:10:27.426916Z","shell.execute_reply":"2025-11-25T05:35:04.276974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Single example","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n\ndef arccos_safe(x):\n    return np.arccos(np.clip(x, -1.0, 1.0))\n\ndef normalize_to_rgb(coords):\n    min_val = np.min(coords)\n    max_val = np.max(coords)\n    if max_val - min_val < 1e-8:\n        return np.zeros_like(coords, dtype=np.uint8)\n    norm_coords = 255.0 * (coords - min_val) / (max_val - min_val + 1e-8)\n    return np.clip(norm_coords, 0, 255).astype(np.uint8)\n\ndef normalize_to_0_255(arr):\n    a = arr.astype(np.float32)\n    mn, mx = a.min(), a.max()\n    if mx - mn < 1e-8:\n        return np.zeros_like(a, dtype=np.uint8)\n    scaled = 255.0 * (a - mn) / (mx - mn)\n    return np.clip(scaled, 0, 255).astype(np.uint8)\n\n\ndef compute_motion_of_joints(skeleton_sequence):\n    Sx = normalize_to_rgb(skeleton_sequence[:, :, 0])\n    Sy = normalize_to_rgb(skeleton_sequence[:, :, 1])\n    Sz = normalize_to_rgb(skeleton_sequence[:, :, 2])\n    motion = np.stack([Sx, Sy, Sz], axis=2)  # (N,J,3)\n    return motion\n\n\ndef get_pair_indices(J):\n    return [(i, j) for i in range(J) for j in range(J) if i != j]\n\ndef compute_pairwise_pot2i_features(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    pairs = get_pair_indices(J)\n    M = len(pairs)\n\n    Ox = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n    Oy = np.array([0.0, 1.0, 0.0], dtype=np.float32)\n    Oz = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n\n    pose_dists = np.zeros((N, M, 3), dtype=np.float32)\n    pose_orients = np.zeros((N, M, 3), dtype=np.float32)\n    trans_dists = np.zeros((N, M, 3), dtype=np.float32)\n    trans_orients = np.zeros((N, M, 3), dtype=np.float32)\n\n    for t in range(N):\n        for idx, (i, j) in enumerate(pairs):\n            u = skeleton_sequence[t, i] - skeleton_sequence[t, j]\n            pose_dists[t, idx, :] = np.abs(u)\n            norm_u = np.linalg.norm(u) + 1e-8\n            u_unit = u / norm_u\n            pose_orients[t, idx, :] = [\n                arccos_safe(np.dot(u_unit, Ox)),\n                arccos_safe(np.dot(u_unit, Oy)),\n                arccos_safe(np.dot(u_unit, Oz))\n            ]\n            if t > 0:\n                v = skeleton_sequence[t, i] - skeleton_sequence[t-1, j]\n                trans_dists[t, idx, :] = np.abs(v)\n                norm_v = np.linalg.norm(v) + 1e-8\n                v_unit = v / norm_v\n                trans_orients[t, idx, :] = [\n                    arccos_safe(np.dot(v_unit, Ox)),\n                    arccos_safe(np.dot(v_unit, Oy)),\n                    arccos_safe(np.dot(v_unit, Oz))\n                ]\n\n    pose_dists = normalize_to_0_255(pose_dists)\n    trans_dists = normalize_to_0_255(trans_dists)\n    pose_orients = normalize_to_0_255(pose_orients)\n    trans_orients = normalize_to_0_255(trans_orients)\n\n    combined = np.concatenate([pose_dists, pose_orients, trans_dists, trans_orients], axis=0)\n    return combined\n\ndef make_PoT2I_image(skeleton_sequence, target_height, target_width):\n    full = compute_pairwise_pot2i_features(skeleton_sequence)\n    resized = cv2.resize(full, (target_width, target_height), interpolation=cv2.INTER_CUBIC)\n    return np.clip(resized, 0, 255).astype(np.uint8)\n\n\ndef concatenate_motion_and_pot(motion, pot_resized):\n    H, W, _ = motion.shape\n    pot_resized = cv2.resize(pot_resized, (W, H), interpolation=cv2.INTER_CUBIC)\n    combined = np.concatenate([motion, pot_resized], axis=1)  # left-right\n\n    mn, mx = combined.min(), combined.max()\n    if mx - mn < 1e-8:\n        norm_img = np.zeros_like(combined, dtype=np.uint8)\n    else:\n        norm_img = 255.0 * (combined - mn) / (mx - mn)\n        norm_img = np.clip(norm_img, 0, 255).astype(np.uint8)\n\n    final = cv2.resize(norm_img, (224, 224), interpolation=cv2.INTER_CUBIC)\n    return final\n\n\ndef load_skeleton_file(filepath):\n    joints = []\n    with open(filepath, 'r') as f:\n        lines = f.read().strip().split('\\n')\n        for line in lines[1:]:\n            if not line.strip():\n                continue\n            parts = line.strip().split(',')\n            if len(parts) < 3:\n                continue\n            x, y, z = float(parts[0]), float(parts[1]), float(parts[2])\n            joints.append([x, y, z])\n    return np.array(joints, dtype=np.float32)\n\ndef load_sequence_skeletons(sequence_path, expected_joints=20):\n    file_list_path = os.path.join(sequence_path, 'fileList.txt')\n    frames = []\n    if not os.path.exists(file_list_path):\n        return None\n    with open(file_list_path, 'r') as f:\n        for line in f:\n            frame_id = line.strip().split()[0]\n            skeleton_file = None\n            for file in os.listdir(sequence_path):\n                if file.startswith(f\"frame_{frame_id}_\") and file.endswith(\"_skeletons.txt\"):\n                    skeleton_file = file\n                    break\n            if skeleton_file is None:\n                continue\n            joints = load_skeleton_file(os.path.join(sequence_path, skeleton_file))\n            if joints.shape[0] == expected_joints:\n                frames.append(joints)\n    if not frames:\n        return None\n    return np.stack(frames)\n\n\nif __name__ == \"__main__\":\n    base_path = '/kaggle/input/n-ucla'\n    output_moj_dir = '/kaggle/working/moj_images'\n    os.makedirs(output_moj_dir, exist_ok=True)\n\n    # Pick one view & one sequence\n    view_folder = sorted(os.listdir(os.path.join(base_path, \"multiview_action\")))[0]\n    seq_folder = sorted(os.listdir(os.path.join(base_path, \"multiview_action\", view_folder)))[0]\n\n    seq_path = os.path.join(base_path, \"multiview_action\", view_folder, seq_folder)\n    skeleton_sequence = load_sequence_skeletons(seq_path, expected_joints=20)\n\n    if skeleton_sequence is not None:\n        motion = compute_motion_of_joints(skeleton_sequence)\n        pot_resized = make_PoT2I_image(skeleton_sequence, target_height=motion.shape[0], target_width=motion.shape[1])\n        moj_image = concatenate_motion_and_pot(motion, pot_resized)\n\n        \n        save_path = os.path.join(output_moj_dir, f\"{view_folder}_{seq_folder}.png\")\n        cv2.imwrite(save_path, cv2.cvtColor(moj_image, cv2.COLOR_RGB2BGR))\n\n        print(\"Saved test image:\", save_path)\n\n        plt.imshow(moj_image)\n        plt.title(\"MOJ + PoT2I (Side-by-Side)\")\n        plt.axis(\"off\")\n        plt.show()\n    else:\n        print(\"No valid skeleton sequence found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:35:42.347031Z","iopub.execute_input":"2025-11-25T05:35:42.347747Z","iopub.status.idle":"2025-11-25T05:35:44.030558Z","shell.execute_reply.started":"2025-11-25T05:35:42.34772Z","shell.execute_reply":"2025-11-25T05:35:44.029808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MOdel","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, regularizers\n\ndef build_moj_cnn(input_shape=(224, 224, 3), num_classes=10, lr=0.0005):\n    \"\"\"\n    Build the proposed lightweight CNN for MOJ descriptors.\n    \n    Args:\n        input_shape: tuple, input image shape (default 224x224x3)\n        num_classes: int, number of action classes\n        lr: float, learning rate (default 0.0005)\n\n    Returns:\n        model: compiled Keras model\n    \"\"\"\n\n    model = models.Sequential()\n\n   \n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\",\n                            input_shape=input_shape,\n                            kernel_initializer=\"he_normal\",\n                            kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n\n    \n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\",\n                            kernel_initializer=\"he_normal\",\n                            kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n\n    \n    model.add(layers.Flatten())\n\n   \n    model.add(layers.Dense(256, kernel_initializer=\"he_normal\"))\n    model.add(layers.ReLU())\n    model.add(layers.Dropout(0.5))\n\n    model.add(layers.Dense(128, kernel_initializer=\"he_normal\"))\n    model.add(layers.ReLU())\n    model.add(layers.Dropout(0.5))\n\n   \n    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n\n    optimizer = optimizers.Adam(learning_rate=lr)\n\n    model.compile(optimizer=optimizer,\n                  loss=\"categorical_crossentropy\",\n                  metrics=[\"accuracy\"])\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:35:48.50968Z","iopub.execute_input":"2025-11-25T05:35:48.510032Z","iopub.status.idle":"2025-11-25T05:35:48.51794Z","shell.execute_reply.started":"2025-11-25T05:35:48.510002Z","shell.execute_reply":"2025-11-25T05:35:48.517134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Normal Train test split","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\n\n\ndef load_moj_dataset(base_dir, target_size=(224,224)):\n    X, y = [], []\n    for fname in tqdm(os.listdir(base_dir), desc=\"Loading MOJ images\"):\n        if not fname.endswith(\".png\"):\n            continue\n        \n        # Example: view_1_a01_s01_e01.png → action = a01\n        parts = fname.split(\"_\")\n        if len(parts) < 2:\n            continue\n        action_label = parts[1]   # \"a01\", \"a02\", ...\n\n        img_path = os.path.join(base_dir, fname)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, target_size)\n\n        X.append(img)\n        y.append(action_label)\n    \n    return np.array(X, dtype=np.float32), np.array(y)\n\ndef train_on_nucla(moj_dir, num_classes=10, batch_size=64, epochs=30):\n    # Load dataset\n    X, y = load_moj_dataset(moj_dir, target_size=(224,224))\n    X = X / 255.0  # normalize\n\n    # Encode labels\n    le = LabelEncoder()\n    y_int = le.fit_transform(y)\n    y_cat = to_categorical(y_int, num_classes=num_classes)\n\n    # Train/test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n    )\n\n    # Build CNN (your function from above)\n    model= build_moj_cnn(input_shape=(224,224,3), num_classes=num_classes)\n\n    # Train\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=1\n    )\n\n    # Evaluate\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\" Test Accuracy: {test_acc:.4f}\")\n\n    return model, history, le, x_test, y_test\n\n\nmoj_dir = \"/kaggle/working/moj_images\"  \nmodel, history, label_encoder, x_test, y_test = train_on_nucla(moj_dir, num_classes=10, epochs=10)\nprint(model.summary(\n    \\\n))\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ny_pred_probs = model.predict(X_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\ncm = confusion_matrix(y_true, y_pred)\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=label_encoder.classes_,\n            yticklabels=label_encoder.classes_)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix - MOJ CNN\")\nplt.show()\n\n\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:35:54.48585Z","iopub.execute_input":"2025-11-25T05:35:54.486409Z","iopub.status.idle":"2025-11-25T05:36:54.479304Z","shell.execute_reply.started":"2025-11-25T05:35:54.486385Z","shell.execute_reply":"2025-11-25T05:36:54.478161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Flattening all of them\n","metadata":{}},{"cell_type":"markdown","source":"### entire dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\n\n\ndef arccos_safe(x):\n    return np.arccos(np.clip(x, -1.0, 1.0))\n\ndef normalize_to_rgb(coords):\n    min_val = np.min(coords)\n    max_val = np.max(coords)\n    norm_coords = 255 * (coords - min_val) / (max_val - min_val + 1e-8)\n    return norm_coords.astype(np.uint8)\n\n\ndef compute_motion_of_joints(skeleton_sequence):\n    Sx = normalize_to_rgb(skeleton_sequence[:, :, 0])\n    Sy = normalize_to_rgb(skeleton_sequence[:, :, 1])\n    Sz = normalize_to_rgb(skeleton_sequence[:, :, 2])\n    return np.stack([Sx, Sy, Sz], axis=2)   # (N,J,3)\n\ndef compute_pose_orientation(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    pose_orientation = []\n    for t in range(N):\n        frame_orients = []\n        for i in range(J):\n            for j in range(J):\n                if i == j: continue\n                diff = skeleton_sequence[t, i] - skeleton_sequence[t, j]\n                norm = np.linalg.norm(diff) + 1e-8\n                normalized = diff / norm\n                frame_orients.append(arccos_safe(normalized))\n        pose_orientation.append(frame_orients)\n    pose_orientation = np.array(pose_orientation)  # (N, J*(J-1), 3)\n    return normalize_to_rgb(pose_orientation)\n\ndef compute_transition_orientation(skeleton_sequence):\n    N, J, _ = skeleton_sequence.shape\n    if N <= 1:\n        # Not enough frames for transition\n        return np.zeros((N, J*(J-1), 3), dtype=np.uint8)\n\n    trans_orientation = []\n    for t in range(1, N):\n        frame_orients = []\n        for i in range(J):\n            for j in range(J):\n                if i == j: continue\n                diff = skeleton_sequence[t, i] - skeleton_sequence[t-1, j]\n                norm = np.linalg.norm(diff) + 1e-8\n                normalized = diff / norm\n                frame_orients.append(arccos_safe(normalized))\n        trans_orientation.append(frame_orients)\n    trans_orientation = np.array(trans_orientation)  # (N-1, J*(J-1), 3)\n    \n    # Pad the first frame with zeros to match motion/pose frame count\n    first_frame = np.zeros_like(trans_orientation[0:1])\n    trans_orientation = np.vstack([first_frame, trans_orientation])\n    return normalize_to_rgb(trans_orientation)\n\n\n\ndef build_action_image(motion, pose, transition, target_size=(224,224)):\n    # Ensure same frame count\n    N = min(motion.shape[0], pose.shape[0], transition.shape[0])\n    motion, pose, transition = motion[:N], pose[:N], transition[:N]\n\n    frames = []\n    for t in range(N):\n        moj_row = motion[t].flatten()\n        pose_row = pose[t].flatten()\n        trans_row = transition[t].flatten()\n        frame_row = np.concatenate([moj_row, pose_row, trans_row], axis=-1)\n        frames.append(frame_row)\n    feature_matrix = np.stack(frames)  # (N, F)\n\n    # Normalize\n    feature_matrix = (feature_matrix - feature_matrix.min()) / (feature_matrix.max() - feature_matrix.min() + 1e-8)\n    feature_matrix = (feature_matrix * 255).astype(np.uint8)\n\n    # Reshape into RGB image\n    W = feature_matrix.shape[1] // 3\n    img = feature_matrix.reshape(N, W, 3)\n\n    # Resize to CNN input size\n    img_resized = cv2.resize(img, target_size)\n    return img_resized\n\n\ndef load_skeleton_file(filepath):\n    joints = []\n    with open(filepath, 'r') as f:\n        lines = f.read().strip().split('\\n')\n        for line in lines[1:]:\n            if not line.strip(): continue\n            parts = line.strip().split(',')\n            if len(parts) < 3: continue\n            x, y, z = float(parts[0]), float(parts[1]), float(parts[2])\n            joints.append([x, y, z])\n    return np.array(joints, dtype=np.float32)\n\ndef load_sequence_skeletons(sequence_path, expected_joints=20):\n    file_list_path = os.path.join(sequence_path, 'fileList.txt')\n    frames = []\n    with open(file_list_path, 'r') as f:\n        for line in f:\n            frame_id = line.strip().split()[0]\n            skeleton_file = None\n            for file in os.listdir(sequence_path):\n                if file.startswith(f\"frame_{frame_id}_\") and file.endswith(\"_skeletons.txt\"):\n                    skeleton_file = file\n                    break\n            if skeleton_file is None: continue\n            joints = load_skeleton_file(os.path.join(sequence_path, skeleton_file))\n            if joints.shape[0] == expected_joints:\n                frames.append(joints)\n    return np.stack(frames) if frames else None\n\ndef process_nucla_dataset(base_path, output_dir):\n    views_dir = os.path.join(base_path, 'multiview_action')\n    os.makedirs(output_dir, exist_ok=True)\n\n    image_paths = []\n    view_folders = [v for v in sorted(os.listdir(views_dir)) if os.path.isdir(os.path.join(views_dir, v))]\n\n    for view_folder in view_folders:\n        view_path = os.path.join(views_dir, view_folder)\n        seq_folders = [s for s in sorted(os.listdir(view_path)) if os.path.isdir(os.path.join(view_path, s))]\n        \n        print(f\"Processing view: {view_folder} ({len(seq_folders)} sequences)\")\n        for seq_folder in tqdm(seq_folders, desc=f\"{view_folder} sequences\"):\n            seq_path = os.path.join(view_path, seq_folder)\n            skeleton_sequence = load_sequence_skeletons(seq_path)\n            if skeleton_sequence is None:\n                continue\n\n            motion = compute_motion_of_joints(skeleton_sequence)\n            pose = compute_pose_orientation(skeleton_sequence)\n            transition = compute_transition_orientation(skeleton_sequence)\n            img = build_action_image(motion, pose, transition)\n\n            save_path = os.path.join(output_dir, f\"{view_folder}_{seq_folder}.png\")\n            cv2.imwrite(save_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n            image_paths.append(save_path)\n    return image_paths\n\ndef visualize_images(image_paths, num_images=5):\n    selected_paths = random.sample(image_paths, min(len(image_paths), num_images))\n    plt.figure(figsize=(15, 3 * num_images))\n    for i, path in enumerate(selected_paths):\n        img = cv2.imread(path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(num_images, 1, i + 1)\n        plt.imshow(img_rgb)\n        plt.title(os.path.basename(path))\n        plt.axis('off')\n    plt.show()\n\nbase_path = '/kaggle/input/n-ucla'\noutput_dir = '/kaggle/working/action_images'\n\nimage_paths = process_nucla_dataset(base_path, output_dir)\nvisualize_images(image_paths, num_images=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T05:49:39.53759Z","iopub.execute_input":"2025-11-25T05:49:39.538381Z","iopub.status.idle":"2025-11-25T06:03:07.574102Z","shell.execute_reply.started":"2025-11-25T05:49:39.538347Z","shell.execute_reply":"2025-11-25T06:03:07.573225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### One image","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n# Pick any one image (first image for example)\nimg_path = image_paths[0]\n\n# Load and convert to RGB\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Plot\nplt.figure(figsize=(6,6))\nplt.imshow(img)\nplt.title(img_path.split('/')[-1])\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:26:50.033671Z","iopub.execute_input":"2025-11-25T06:26:50.033903Z","iopub.status.idle":"2025-11-25T06:26:50.209533Z","shell.execute_reply.started":"2025-11-25T06:26:50.033881Z","shell.execute_reply":"2025-11-25T06:26:50.208872Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MOdel","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, regularizers\n\ndef build_moj_cnn(input_shape=(224, 224, 3), num_classes=10, lr=0.0005):\n    \"\"\"\n    Build the proposed lightweight CNN for MOJ descriptors.\n\n    Args:\n        input_shape: tuple, input image shape (default 224x224x3)\n        num_classes: int, number of action classes\n        lr: float, learning rate (default 0.0005)\n\n    Returns:\n        model: compiled Keras model\n    \"\"\"\n\n    model = models.Sequential()\n\n   \n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\",\n                            input_shape=input_shape,\n                            kernel_initializer=\"he_normal\",\n                            kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\",\n                            kernel_initializer=\"he_normal\",\n                            kernel_regularizer=regularizers.l2(1e-4)))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n\n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(256, kernel_initializer=\"he_normal\"))\n    model.add(layers.ReLU())\n    model.add(layers.Dropout(0.5))\n\n    model.add(layers.Dense(128, kernel_initializer=\"he_normal\"))\n    model.add(layers.ReLU())\n    model.add(layers.Dropout(0.5))\n\n\n    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n\n\n    optimizer = optimizers.Adam(learning_rate=lr)\n\n\n    model.compile(optimizer=optimizer,\n                  loss=\"categorical_crossentropy\",\n                  metrics=[\"accuracy\"])\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:27:16.529379Z","iopub.execute_input":"2025-11-25T06:27:16.529925Z","iopub.status.idle":"2025-11-25T06:27:16.537574Z","shell.execute_reply.started":"2025-11-25T06:27:16.5299Z","shell.execute_reply":"2025-11-25T06:27:16.536976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train and test","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay   ### >>> ADDED LINE\n\n\ndef load_moj_dataset(base_dir, target_size=(224,224)):\n    X, y = [], []\n    for fname in tqdm(os.listdir(base_dir), desc=\"Loading MOJ images\"):\n        if not fname.endswith(\".png\"):\n            continue\n        \n        parts = fname.split(\"_\")\n        if len(parts) < 2:\n            continue\n        action_label = parts[1]   # \"a01\"\n\n        img_path = os.path.join(base_dir, fname)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, target_size)\n\n        X.append(img)\n        y.append(action_label)\n    \n    return np.array(X, dtype=np.float32), np.array(y)\n\n\ndef train_on_nucla(moj_dir, num_classes=10, batch_size=64, epochs=30):\n    X, y = load_moj_dataset(moj_dir, target_size=(224,224))\n    X = X / 255.0  \n\n    le = LabelEncoder()\n    y_int = le.fit_transform(y)\n    y_cat = to_categorical(y_int, num_classes=num_classes)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n    )\n\n    model = build_moj_cnn(input_shape=(224,224,3), num_classes=num_classes)\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=1\n    )\n\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\" Test Accuracy: {test_acc:.4f}\")\n\n    model.summary()   \n    \n    y_pred_probs = model.predict(X_test)                    \n    y_pred = np.argmax(y_pred_probs, axis=1)                \n    y_true = np.argmax(y_test, axis=1)                      \n    class_names = le.classes_                               \n\n    cm = confusion_matrix(y_true, y_pred)                   \n    plt.figure(figsize=(10, 8))                             \n    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)  \n    disp.plot(xticks_rotation=45, cmap=\"Blues\")             \n    plt.title(\"NUCLA MOJ Confusion Matrix\")                 \n    plt.show()                                             \n    \n\n    return model, history, le\n\n\nmoj_dir = \"/kaggle/working/action_images\"\nmodel, history, label_encoder = train_on_nucla(moj_dir, num_classes=10, epochs=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T06:35:29.819859Z","iopub.execute_input":"2025-11-25T06:35:29.820433Z","iopub.status.idle":"2025-11-25T06:35:29.824692Z","shell.execute_reply.started":"2025-11-25T06:35:29.820411Z","shell.execute_reply":"2025-11-25T06:35:29.824084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LOSO","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\n\nIMG_DIR = \"/kaggle/working/action_images\"\nIMG_SIZE = (224, 224)\nNUM_CLASSES = 10   # number of action classes in dataset\nEPOCHS = 10\nBATCH_SIZE = 32\n\n\ndef parse_filename(filename):\n    \"\"\"\n    Extract view, action, subject, execution from filename\n    Example: view_1_a03_s10_e03.png\n    \"\"\"\n    match = re.match(r\"view_(\\d+)_a(\\d+)_s(\\d+)_e(\\d+)\\.png\", filename)\n    if match:\n        view, action, subject, execution = map(int, match.groups())\n        return view, action, subject, execution\n    return None\n\ndef build_action_map():\n    \"\"\"Build a mapping from actual action numbers to continuous 0..NUM_CLASSES-1\"\"\"\n    action_ids = set()\n    for fname in os.listdir(IMG_DIR):\n        parsed = parse_filename(fname)\n        if parsed:\n            _, action, _, _ = parsed\n            action_ids.add(action)\n    action_ids = sorted(list(action_ids))\n    action_map = {aid: idx for idx, aid in enumerate(action_ids)}\n    return action_map\n\ndef load_dataset(img_dir=IMG_DIR, img_size=IMG_SIZE, action_map=None):\n    X, y, subjects = [], [], []\n    for fname in os.listdir(img_dir):\n        if not fname.endswith(\".png\"):\n            continue\n        parsed = parse_filename(fname)\n        if not parsed:\n            continue\n        view, action, subject, execution = parsed\n        img_path = os.path.join(img_dir, fname)\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, img_size)\n        img = img / 255.0\n        X.append(img)\n        y.append(action_map[action])  # map to 0..NUM_CLASSES-1\n        subjects.append(subject)\n    return np.array(X), np.array(y), np.array(subjects)\n\n\ndef run_single_subject_experiment(test_subject=1):\n    # Build action mapping\n    action_map = build_action_map()\n    print(\"Action mapping:\", action_map)\n\n    X, y, subjects = load_dataset(action_map=action_map)\n\n    # Split by subject\n    train_idx = subjects != test_subject\n    test_idx = subjects == test_subject\n\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n    # One-hot encode labels\n    y_train = to_categorical(y_train, NUM_CLASSES)\n    y_test = to_categorical(y_test, NUM_CLASSES)\n\n    # Build fresh model\n    model = build_moj_cnn(input_shape=(224,224,3), num_classes=NUM_CLASSES)\n\n    # Train\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        verbose=2\n    )\n\n    # Evaluate\n    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"\\nFinal Accuracy for subject {test_subject}: {acc:.4f}\")\n    return acc\n\nacc = run_single_subject_experiment(test_subject=5)   \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T07:50:48.503421Z","iopub.execute_input":"2025-10-06T07:50:48.503719Z","iopub.status.idle":"2025-10-06T07:51:59.257355Z","shell.execute_reply.started":"2025-10-06T07:50:48.503697Z","shell.execute_reply":"2025-10-06T07:51:59.256578Z"}},"outputs":[],"execution_count":null}]}